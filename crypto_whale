\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{\textbf{Cryptocurrency Price Prediction using Twitter Interactions and Whale Tracking}\\
    \author{\textbf{Cao-Duy Vo}$^{1}$, \textbf{Xuan-Hien Nguyen}$^{1}$\\
    $^{1}$ University of Science, VNU-HCM, Ho Chi Minh City, Vietnam \\
    $^{2}$ Vietnam National University, Ho Chi Minh City, Vietnam
    }
}

\maketitle

\begin{abstract}
abstract \\
deadline: t7 2024-03-17 sau khi mn nop \\
pic: duy \\
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
deadline: t7 2024-03-17 sau khi mn nop \\
pic: duy

\section{RELATED WORKS}
deadline: 2024-03-16 19h \\
pic: hien
\subsection{LSTM}
There are many RNN-based stock prediction methods that is used to predict stock trend
by using technical data or sentiment data or a combination of both. However,in this paper,
we focus on LSTM.

Firstly, LSTM (Long Short-Term Memory)[8] has long been used as an alternative to basic
(RNN) Recurrent Neural Networks for time series prediction tasks with the main issue
 is that RNNs can theoretically store recent information but existing learning algorithms
  struggle with long time lags between inputs and desired outputs, these algorithms
  are slow or impractical, offering no clear benefit over simpler networks.

 LSTM is a new RNN architecture designed to address the vanishing gradient problem
 by introducing special units with "constant error carousels" that allow error to flow through the network for extended periods so then, it can learn complex patterns even with noisy data and long time lags (over 1000 steps). These units are controlled by gates that regulate information flow. The forget gate decides what information is remembered for
future predictions, the input gate determines the importance of
the input, and the output gate uses information passed through
the previous two gates to output the next hidden state of the
model.

The calculation is as follows:

(1) Input the output value of the previous unit and the input value of the current time into the
forgetting gate, and decide which information is allowed to be retained and which is abandoned after
calculation. Such as the following formula:
\begin{align}
    {f_t} = {\sigma}(W_t \cdot [h_t, x_t] + b_f) && (1)
\end{align}

where $f_t$ indicates that the output ranges from 0 to 1, 0 indicates that the information is abandoned,
1 indicates that the information is retained, $h_{t-1}$ represents the output of the previous unit,
$x_t$ represents the input data at the current time, $W_f$ represents the weight of the forgetting gate, and $b_f$ represents the deviation of the forgetting gate.
(2)Then decide what new information can be retained. There are two parts here. The first part is
the sigmoid layer, also known as the input gate, which determines which newly entered information
can be updated. The second part is that the tanh layer is designed to generate new information, where
a new vector of candidate values is created. The formula is as follows:
\begin{align}
i_t &= \sigma(W_i \cdot [h_{t-1}, x_i] + b_i)
C_t &= \sigma(W_c \cdot [h_{t-1}, x_t] + b_c)
\end{align}
where $i_t$ is a control signal that ranges from 0 to 1, $w_i$ represents the weight of the input gate and
$b_i$ represents the deviation of the input gate, $W_c$ represents the weight of the candidate input gate and
$b_f$ represents the deviation of the candidate input gate.

(3) Renew the state of old cells:
\begin{align}
    C_t = f_t \cdot C_{t-1} + i_t \cdot \widetilde{C_t}
\end{align}

where the old state is multiplied by $f_t$ to
determine the discarded information, and $i_t$ âˆ— $C_t$ is the new candidate value, which changes with the updated state.

(4) Determine the output value. The sigmoid layer determines which cell morphology is exported,
and then it is processed by tanh layer and exported. The formula is shown below:

\begin{align}
    o_t = \sigma(W_o \cdot [h_{t-1}, x_i] + b_o)
\end{align}
\begin{align}
    h_t = o_t * tanh (C_t)
\end{align}

where $W_o$ represents the weight of the output gate and $b_o$ represents the deviation of the output gate.


\subsection{ALSTM}

Jijie Wang et al. [19] introduce ALSTM (The attention mechanism into the LSTM)
model in order to improve the prediction performance of the proposed
model to analyse and predict Asian stock markets closing index.
The importance of input data is fully utilized to determine the weight
 assignment, attention layer is added in long short-term memory model
  so as to increase the weights of effective and improve the
  prediction accuracy of the model.

  \subsection{Twitter Sentiment analysis}
  T. Swathi et al [20] proposed  sentiment analysis of the social media data of
  stock prices helps to predict future stock prices effectively. Because
  the growth of the internet and social network enables the clients to express
  their opinions and shares their views on future stock processes.
  Besides, the LSTM model is applied to classify tweets into
  positive and negative sentiments related to stock prices. They help investigate
  how the tweets correlate with the nature of the stock market prices.

  \subsection{Crypto Price Analysis}
  There are many people doing research about the prediction
of cryptocurrency. Greaves et al. [21] is a proposed technique
using Logistic Regression and SVM and analyzed using Graph
to predict bitcoin price. Huisu Jang et al. [22] they concern
about a study on modeling and prediction bitcoin with
Bayesian Neural Network and giving some knowledge about
bitcoin. Edwin sin et al. [23] provide topic Bitcoin price
prediction using Ensemble of Neural Networks.

Chih-Hung et al. [24] are
created new forecasting framework bitcoin price using LSTM,
they proposed with two various LSTM models (conventional
LSTM and LSTM with AR(2) model) with 208 records dataset,
compared with MSE, RMSE, MAE, and MAPE. Fei Qian et
al.

The researches above proposed various method to
prediction bitcoin. In this paper, we analyze and constructing a
model to predict crypto using LSTM.

\section{PROPOSED METHOD}
\subsection{Dataset Description and Analysis}
% \label{AA}
deadline: 2024-03-16 19h \\
pic: duy 

\subsection{Architecture Descriptions}
deadline: 2024-03-16 19h \\
pic: vuong
nho Vuong hoac Phuc ve giup cai Architecture
\begin{itemize}
\item doan 1
\item doan 2
\item doan 3
\end{itemize}
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}


\section{EXPERIMENT AND RESULTS ANALYSIS}
duy viet 
deadline: 2024-03-16
\subsection{Metrics}
\subsection{Best Run Parameters}
\subsection{Best Model Parameters}
deadline: 2024-03-16 19h \\
pic: duy 
\subsection{Experimental Results}
\subsection{Comparison price prediction between LSTM and Transformer}
so sanh


\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: image description


\section{CONCLUSION}
pic: duy
deadline: 2024-03-23 duy viet mn review



\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}


\end{document}

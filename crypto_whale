\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{\textbf{Cryptocurrency Price Prediction using Twitter Interactions and Whale Tracking}\\
    \author{\textbf{Cao-Duy Vo}$^{1}$, \textbf{Xuan-Hien Nguyen}$^{1}$\\
    $^{1}$ University of Science, VNU-HCM, Ho Chi Minh City, Vietnam \\
    $^{2}$ Vietnam National University, Ho Chi Minh City, Vietnam
    }
}

\maketitle

\begin{abstract}
abstract \\
deadline: t7 2024-03-17 sau khi mn nop \\
pic: duy \\
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
deadline: t7 2024-03-17 sau khi mn nop \\
pic: duy

\section{RELATED WORKS}
viet code o day


\section{PROPOSED METHOD}
\subsection{Dataset Description and Analysis}
% \label{AA}
deadline: 2024-03-16 19h \\
pic: duy 

\subsection{Architecture Descriptions}
deadline: 2024-03-16 19h \\
pic: vuong 
nho Vuong hoac Phuc ve giup cai Architecture
\begin{itemize}
\item doan 1
\item doan 2
\item doan 3
\end{itemize}
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}


\section{EXPERIMENT AND RESULTS ANALYSIS}
To facilitate objective comparison with existing related work. We focus on utilizing a window of n days to forecast the highest price on day (n+1). To evaluate the models' capabilities in capturing both long-term and short-term dependencies in the stock price data, we conducted experiments by varying the value of n, which is commonly referred to as the "window of days." This methodological approach enables us to directly compare our findings with those of previous studies, ensuring the validity and reliability of our results.
\subsection{Metrics}
For measuring loss, we utilized the Mean Squared Error (MSE) loss function, given that predicting the highest price on day (n+1) is a regression-based task. MSE is calculated by computing the average of the squared differences between each data point's corresponding ground truth label and the model's predicted output. The mathematical formula for MSE loss is presented in Equation \ref{eq:mse} below:
\begin{equation}
\label{eq:mse}
\text{MSE loss} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y_i} - y_i)^2
\end{equation}
Mean Absolute Percentage Error (MAPE) serves as a crucial metric for evaluating the precision of forecasting methodologies. It entails calculating the average of the absolute percentage errors associated with each data point within a dataset. This metric quantifies the accuracy of forecasted values in relation to their corresponding actual values. MAPE proves particularly valuable when analyzing extensive datasets and necessitates the utilization of non-zero values within the dataset. 
\begin{equation}
\label{eq:mape}
\text{MAPE} = \frac{1}{\text{N}} \sum_{i=1}^{\text{N}} \left( \frac{\left| {y}_i - \hat{y_i} \right|}{\left| {y}_i \right|} \right) \times 100
\end{equation}
\subsection{Best Run Parameters}
In all our tests, the model was trained for 100 epochs. We opted for the Adam optimizer throughout due to its well-established reputation for achieving rapid convergence. Lastly, a learning rate of 0.0001 demonstrably gave the best convergence results across all trials.
\subsection{Best Model Parameters}
For our Stock Attention model, we utilized a combination of LSTM and attention mechanisms to effectively capture temporal dependencies in the data while focusing on relevant information. The model architecture consisted of 64 LSTM layers stacked on top of each other. To enhance generalization and prevent overfitting, we incorporated a dropout value of 0.1. Dropout randomly drops a fraction of input units during training, which encourages the model to learn more robust features and reduces reliance on specific inputs. This configuration allowed for a comprehensive understanding of the sequential patterns present in the stock data.

We leveraged the power of attention mechanisms by employing 128 attention heads. Each attention head has its own set of weights, enabling the model to focus on different aspects of the input sequence simultaneously. This multi-head attention mechanism provides the model with a richer representation of contextual information, leading to more informed predictions.

Additionally, Dense layers with 32 neurons each follow the attention layer. And finally utilize Leaky ReLU activation functions with a chosen alpha value of 0.4 for enhanced nonlinear transformation capabilities.

By combining LSTM layers with attention mechanisms and carefully tuning hyperparameters such as the number of layers, hidden dimensions, dropout rate, and attention heads, our Stock Attention model achieved superior performance across various time windows, demonstrating its efficacy in capturing and leveraging temporal dynamics in stock data for accurate forecasting.
\subsection{Experimental Results}
As highlighted in the Related Works section, Paper A presents a baseline LSTM model for forecasting with a time window of 30 days. We commenced our investigation by replicating the architecture outlined in Paper A, utilizing a feature set encompassing solely Twitter interactions and price data (totaling 7 features). This initial configuration yielded a noteworthy improvement, achieving a Mean Absolute Percentage Error (MAPE) 4.5\% higher compared to the baseline established by the previous research.

We then exploratory incorporated an additional feature – whale tracking data – into the model. While this inclusion did not lead to a further reduction in MAPE, the performance remained on par with the previous baseline ($\sim3.6\% MAPE).

Undeterred, we explored the potential of a novel architecture – the Stock Attention model. This innovative approach yielded exceptional results, surpassing all prior research by achieving a remarkable MAPE of 2.1\% lower than the established benchmark.

\subsection{Comparison price prediction between LSTM and Transformer}
so sanh


\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: image description


\section{CONCLUSION}
pic: duy
deadline: 2024-03-23 duy viet mn review



\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}


\end{document}
